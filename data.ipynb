{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "## Major\n",
    "- [x] create term-by-document matrix (calculate words frequncies for each term-document pair)\n",
    " - [ ] check that it's actually correct - especially if we don't map terms to wrong documents\n",
    "- [x] convert term-by-document frequencies to tf-idf (calcualte tf-idf for each term-document pair)\n",
    " - [ ] check\n",
    "- [ ] we may need actual (numpy?) matrix?\n",
    "- [ ] LSI magic\n",
    "\n",
    "### Minor\n",
    "- [x] remove numbers from terms - done but not sure if it's good thing to do, maybe it's also important for relevancy of docs,\n",
    "like for example when there is year written?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data = pd.read_csv(\"articles.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Justin Lee</td>\n",
       "      <td>8.3K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.com/swlh/chatbots-were-the-next...</td>\n",
       "      <td>Chatbots were the next big thing: what happene...</td>\n",
       "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author claps  reading_time  \\\n",
       "0  Justin Lee  8.3K            11   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://medium.com/swlh/chatbots-were-the-next...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Chatbots were the next big thing: what happene...   \n",
       "\n",
       "                                                text  \n",
       "0  Oh, how the headlines blared:\\nChatbots were T...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_docs(docs, use_lemmatizer = True):\n",
    "    '''Tokenize and preprocess documents\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    use_lemmatizer : bool\n",
    "                     Uses lemmazizer if True, othrerwise uses stemmer.\n",
    "    '''\n",
    "    preproccessed_docs = []\n",
    "    \n",
    "    # English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # Word tokenizer that removes punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # lemmatizer / Stemmer\n",
    "    if use_lemmatizer:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    else:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    for row in docs.itertuples(index=True, name='Doc'):\n",
    "        text = row.text\n",
    "        \n",
    "        # remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        text_words = tokenizer.tokenize(text)\n",
    "        \n",
    "        if use_lemmatizer:\n",
    "            text_words = [lemmatizer.lemmatize(word, pos=\"v\").lower() for word in text_words\n",
    "                          if word not in string.punctuation and word.lower() not in en_stop]\n",
    "        else:\n",
    "            text_words = [stemmer.stem(word).lower() for word in text_words\n",
    "                         if word not in string.punctuation and word.lower() not in en_stop]\n",
    "        \n",
    "        preproccessed_docs.append({'words': text_words})\n",
    "    \n",
    "    pdocs = pd.DataFrame(preproccessed_docs)\n",
    "    return pdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[oh, headline, blare, chatbots, next, big, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ever, find, look, question, concept, syntax, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[machine, learn, increasingly, move, hand, des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[understand, machine, learning, big, question,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[want, learn, apply, artificial, intelligence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>[click, share, article, linkedin, skip, part, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>[opinions, deep, neural, network, machine, lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>[everyone, remotely, tune, recent, progress, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>[one, biggest, misconceptions, around, idea, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>[believe, psychologist, philosopher, brain, li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 words\n",
       "0    [oh, headline, blare, chatbots, next, big, thi...\n",
       "1    [ever, find, look, question, concept, syntax, ...\n",
       "2    [machine, learn, increasingly, move, hand, des...\n",
       "3    [understand, machine, learning, big, question,...\n",
       "4    [want, learn, apply, artificial, intelligence,...\n",
       "..                                                 ...\n",
       "332  [click, share, article, linkedin, skip, part, ...\n",
       "333  [opinions, deep, neural, network, machine, lea...\n",
       "334  [everyone, remotely, tune, recent, progress, m...\n",
       "335  [one, biggest, misconceptions, around, idea, d...\n",
       "336  [believe, psychologist, philosopher, brain, li...\n",
       "\n",
       "[337 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preproccessed_docs = preprocess_docs(bp_data)\n",
    "display(preproccessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_by_document_frequency(preprocessed_docs):\n",
    "    document_by_term = {}\n",
    "    \n",
    "    for index, row in preprocessed_docs.iterrows():\n",
    "        doc_id = index\n",
    "        doc_words = row['words']\n",
    "        \n",
    "        document_by_term[doc_id] = {\n",
    "            'total_words': len(doc_words)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        for word in set(row['words']):\n",
    "            document_by_term[doc_id][word] = doc_words.count(word)\n",
    "\n",
    "    df = pd.DataFrame(document_by_term)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequency = get_term_by_document_frequency(preproccessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_words</th>\n",
       "      <td>1121.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>1241.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>1267.0</td>\n",
       "      <td>1612.0</td>\n",
       "      <td>585.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>1626.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3394.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>795.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>1032.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regularisation</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oncologist</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intelligible</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loopy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wavy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16604 rows Ã— 337 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0      1       2      3       4       5      6      7    \\\n",
       "total_words     1121.0  699.0  1241.0  713.0  1267.0  1612.0  585.0  503.0   \n",
       "way                9.0    1.0     1.0    2.0     4.0     4.0    2.0    2.0   \n",
       "help               2.0    1.0     2.0    2.0     2.0     NaN    3.0    1.0   \n",
       "live               1.0    1.0     NaN    NaN     NaN     NaN    NaN    2.0   \n",
       "matt               1.0    NaN     NaN    NaN     NaN     NaN    NaN    NaN   \n",
       "...                ...    ...     ...    ...     ...     ...    ...    ...   \n",
       "regularisation     NaN    NaN     NaN    NaN     NaN     NaN    NaN    NaN   \n",
       "oncologist         NaN    NaN     NaN    NaN     NaN     NaN    NaN    NaN   \n",
       "intelligible       NaN    NaN     NaN    NaN     NaN     NaN    NaN    NaN   \n",
       "loopy              NaN    NaN     NaN    NaN     NaN     NaN    NaN    NaN   \n",
       "wavy               NaN    NaN     NaN    NaN     NaN     NaN    NaN    NaN   \n",
       "\n",
       "                  8       9    ...     327    328     329    330    331  \\\n",
       "total_words     524.0  1626.0  ...  3394.0  673.0  1042.0  433.0  795.0   \n",
       "way               2.0     9.0  ...    20.0    NaN    11.0    NaN    5.0   \n",
       "help              NaN     9.0  ...     1.0    1.0     4.0    1.0    NaN   \n",
       "live              NaN     2.0  ...     NaN    1.0     1.0    NaN    NaN   \n",
       "matt              NaN     NaN  ...     NaN    NaN     NaN    NaN    NaN   \n",
       "...               ...     ...  ...     ...    ...     ...    ...    ...   \n",
       "regularisation    NaN     NaN  ...     NaN    NaN     NaN    NaN    NaN   \n",
       "oncologist        NaN     NaN  ...     NaN    NaN     NaN    NaN    NaN   \n",
       "intelligible      NaN     NaN  ...     NaN    NaN     NaN    NaN    NaN   \n",
       "loopy             NaN     NaN  ...     NaN    NaN     NaN    NaN    NaN   \n",
       "wavy              NaN     NaN  ...     NaN    NaN     NaN    NaN    NaN   \n",
       "\n",
       "                  332     333    334    335     336  \n",
       "total_words     887.0  1011.0  424.0  625.0  1032.0  \n",
       "way               1.0     1.0    NaN    1.0     7.0  \n",
       "help              NaN     3.0    NaN    NaN     5.0  \n",
       "live              NaN     4.0    NaN    NaN     NaN  \n",
       "matt              NaN     NaN    NaN    NaN     NaN  \n",
       "...               ...     ...    ...    ...     ...  \n",
       "regularisation    NaN     NaN    NaN    NaN     3.0  \n",
       "oncologist        NaN     NaN    NaN    NaN     1.0  \n",
       "intelligible      NaN     NaN    NaN    NaN     1.0  \n",
       "loopy             NaN     NaN    NaN    NaN     1.0  \n",
       "wavy              NaN     NaN    NaN    NaN     1.0  \n",
       "\n",
       "[16604 rows x 337 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(df_frequency):\n",
    "    df = df_frequency.copy()\n",
    "    # tf := word frequency / total frequency\n",
    "    df = df.drop('total_words', inplace=False)[:] / df.loc['total_words']\n",
    "    \n",
    "    # idf := log ( len(all_documents) / len(documents_containing_word) )\n",
    "    \n",
    "    corpus_size = df.shape[1]\n",
    "\n",
    "    # number of non-zero cols\n",
    "    df['doc_frequency'] = df.fillna(0).astype(bool).sum(axis=1)\n",
    "        \n",
    "    df['idf'] = np.log( corpus_size / df['doc_frequency'] )\n",
    "    \n",
    "    # tf-idf := tf * idf\n",
    "    _cols = df.columns.difference(['idf', 'doc_frequency'])\n",
    "    df[_cols] = df[_cols].multiply(df[\"idf\"], axis=\"index\")\n",
    "    \n",
    "    df.drop(columns=['doc_frequency', 'idf'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>...</td>\n",
       "      <td>1.574092e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>...</td>\n",
       "      <td>1.269921e-04</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live</th>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matt</th>\n",
       "      <td>0.003955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enjoy</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>8.755953e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regularisation</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oncologist</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intelligible</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loopy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wavy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16603 rows Ã— 337 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0         1         2         3         4         5    \\\n",
       "way             0.002145  0.000382  0.000215  0.000749  0.000843  0.000663   \n",
       "help            0.000769  0.000617  0.000695  0.001209  0.000680       NaN   \n",
       "live            0.001057  0.001696       NaN       NaN       NaN       NaN   \n",
       "matt            0.003955       NaN       NaN       NaN       NaN       NaN   \n",
       "enjoy           0.000003  0.000009  0.000002  0.000004  0.000002  0.000004   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       "regularisation       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "oncologist           NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "intelligible         NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "loopy                NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "wavy                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "                     6         7         8         9    ...           327  \\\n",
       "way             0.000913  0.001062  0.001020  0.001479  ...  1.574092e-03   \n",
       "help            0.002210  0.000857       NaN  0.002386  ...  1.269921e-04   \n",
       "live                 NaN  0.004713       NaN  0.001458  ...           NaN   \n",
       "matt                 NaN       NaN       NaN       NaN  ...           NaN   \n",
       "enjoy           0.000005  0.000024  0.000006  0.000002  ...  8.755953e-07   \n",
       "...                  ...       ...       ...       ...  ...           ...   \n",
       "regularisation       NaN       NaN       NaN       NaN  ...           NaN   \n",
       "oncologist           NaN       NaN       NaN       NaN  ...           NaN   \n",
       "intelligible         NaN       NaN       NaN       NaN  ...           NaN   \n",
       "loopy                NaN       NaN       NaN       NaN  ...           NaN   \n",
       "wavy                 NaN       NaN       NaN       NaN  ...           NaN   \n",
       "\n",
       "                     328       329       330       331       332       333  \\\n",
       "way                  NaN  0.002820       NaN  0.001680  0.000301  0.000264   \n",
       "help            0.000640  0.001655  0.000995       NaN       NaN  0.001279   \n",
       "live            0.001761  0.001138       NaN       NaN       NaN  0.004690   \n",
       "matt                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "enjoy           0.000004  0.000003  0.000007  0.000004  0.000003  0.000003   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       "regularisation       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "oncologist           NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "intelligible         NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "loopy                NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "wavy                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "                     334       335       336  \n",
       "way                  NaN  0.000427  0.001812  \n",
       "help                 NaN       NaN  0.002088  \n",
       "live                 NaN       NaN       NaN  \n",
       "matt                 NaN       NaN       NaN  \n",
       "enjoy           0.000014  0.000005  0.000003  \n",
       "...                  ...       ...       ...  \n",
       "regularisation       NaN       NaN  0.016919  \n",
       "oncologist           NaN       NaN  0.005640  \n",
       "intelligible         NaN       NaN  0.005640  \n",
       "loopy                NaN       NaN  0.005640  \n",
       "wavy                 NaN       NaN  0.005640  \n",
       "\n",
       "[16603 rows x 337 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tf_idf = get_tf_idf(df_frequency)\n",
    "display(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00214461, 0.00038215, 0.00021525, ..., 0.        , 0.0004274 ,\n",
       "        0.00181188],\n",
       "       [0.00076898, 0.00061661, 0.00069462, ..., 0.        , 0.        ,\n",
       "        0.00208823],\n",
       "       [0.00105741, 0.00169579, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00563962],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00563962],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00563962]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = df_tf_idf.fillna(0).to_numpy()\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
