{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "## Major\n",
    "- [x] create term-by-document matrix (calculate words frequncies for each term-document pair)\n",
    " - [ ] check that it's actually correct - especially if we don't map terms to wrong documents\n",
    "- [x] convert term-by-document frequencies to tf-idf (calcualte tf-idf for each term-document pair)\n",
    " - [ ] check\n",
    "- [ ] we may need actual (numpy?) matrix?\n",
    "- [ ] LSI magic\n",
    "\n",
    "### Minor\n",
    "- [x] remove numbers from terms - done but not sure if it's good thing to do, maybe it's also important for relevancy of docs,\n",
    "like for example when there is year written?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data = pd.read_csv(\"articles.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Justin Lee</td>\n",
       "      <td>8.3K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.com/swlh/chatbots-were-the-next...</td>\n",
       "      <td>Chatbots were the next big thing: what happene...</td>\n",
       "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author claps  reading_time  \\\n",
       "0  Justin Lee  8.3K            11   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://medium.com/swlh/chatbots-were-the-next...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Chatbots were the next big thing: what happene...   \n",
       "\n",
       "                                                text  \n",
       "0  Oh, how the headlines blared:\\nChatbots were T...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_docs(docs, use_lemmatizer = True):\n",
    "    '''Tokenize and preprocess documents\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    use_lemmatizer : bool\n",
    "                     Uses lemmazizer if True, othrerwise uses stemmer.\n",
    "    '''\n",
    "    preproccessed_docs = []\n",
    "    \n",
    "    # English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # Word tokenizer that removes punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # lemmatizer / Stemmer\n",
    "    if use_lemmatizer:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    else:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    for row in docs.itertuples(index=True, name='Doc'):\n",
    "        text = row.text\n",
    "        \n",
    "        # remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        text_words = tokenizer.tokenize(text)\n",
    "        \n",
    "        if use_lemmatizer:\n",
    "            text_words = [lemmatizer.lemmatize(word, pos=\"v\").lower() for word in text_words\n",
    "                          if word not in string.punctuation and word.lower() not in en_stop]\n",
    "        else:\n",
    "            text_words = [stemmer.stem(word).lower() for word in text_words\n",
    "                         if word not in string.punctuation and word.lower() not in en_stop]\n",
    "        \n",
    "        preproccessed_docs.append({'words': text_words})\n",
    "    \n",
    "    pdocs = pd.DataFrame(preproccessed_docs)\n",
    "    return pdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[oh, headline, blare, chatbots, next, big, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ever, find, look, question, concept, syntax, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[machine, learn, increasingly, move, hand, des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[understand, machine, learning, big, question,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[want, learn, apply, artificial, intelligence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>[click, share, article, linkedin, skip, part, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>[opinions, deep, neural, network, machine, lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>[everyone, remotely, tune, recent, progress, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>[one, biggest, misconceptions, around, idea, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>[believe, psychologist, philosopher, brain, li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 words\n",
       "0    [oh, headline, blare, chatbots, next, big, thi...\n",
       "1    [ever, find, look, question, concept, syntax, ...\n",
       "2    [machine, learn, increasingly, move, hand, des...\n",
       "3    [understand, machine, learning, big, question,...\n",
       "4    [want, learn, apply, artificial, intelligence,...\n",
       "..                                                 ...\n",
       "332  [click, share, article, linkedin, skip, part, ...\n",
       "333  [opinions, deep, neural, network, machine, lea...\n",
       "334  [everyone, remotely, tune, recent, progress, m...\n",
       "335  [one, biggest, misconceptions, around, idea, d...\n",
       "336  [believe, psychologist, philosopher, brain, li...\n",
       "\n",
       "[337 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preproccessed_docs = preprocess_docs(bp_data)\n",
    "display(preproccessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_by_document_frequency(preprocessed_docs):\n",
    "    document_by_term = {}\n",
    "    \n",
    "    for index, row in preprocessed_docs.iterrows():\n",
    "        doc_id = index\n",
    "        doc_words = row['words']\n",
    "        \n",
    "        # computed later, @TODO: move computation here and fix below or remove\n",
    "#         document_by_term[doc_id] = {\n",
    "#             'total_words': len(doc_words)\n",
    "#         }\n",
    "        document_by_term[doc_id] = {}\n",
    "        \n",
    "        for word in set(row['words']):\n",
    "            document_by_term[doc_id][word] = doc_words.count(word)\n",
    "\n",
    "    df = pd.DataFrame(document_by_term)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-dc35118d4cd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_frequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_term_by_document_frequency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreproccessed_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-c45a8821fd26>\u001b[0m in \u001b[0;36mget_term_by_document_frequency\u001b[1;34m(preprocessed_docs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mdocument_by_term\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_by_term\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "df_frequency = get_term_by_document_frequency(preproccessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_terms(df_frequency, max_df=1, min_df=0, max_terms=None):\n",
    "    '''Remove unimportant terms from term-by-document matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "    max_df : float , between [0, 1]\n",
    "             Terms that appear in more % of documents will be ignored\n",
    "    min_df : float , between [0, 1]\n",
    "             Terms that appear in less % of documents will be ignored\n",
    "    max_terms : int , None\n",
    "                If not None, only top `max_terms` terms will be returned.\n",
    "    '''\n",
    "    df = df_frequency.copy()\n",
    "    if 'doc_frequency' in df:\n",
    "        df = df.drop(columns='doc_frequency')\n",
    "    \n",
    "    corpus_size = df.shape[1]\n",
    "    \n",
    "    df['doc_frequency'] = df_frequency.fillna(0).astype(bool).sum(axis=1) / corpus_size\n",
    "    \n",
    "    total_words = df.loc['total_words']\n",
    "    \n",
    "    df = df[df.doc_frequency <= max_df]\n",
    "    df = df[df.doc_frequency >= min_df]\n",
    "    \n",
    "    if max_terms is not None:\n",
    "        assert('not implementd' == False) # @TODO - implement or remove\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_terms(df_frequency).sort_values('doc_frequency', ascending=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_terms(df_frequency, 0.8, 0.1).sort_values('doc_frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = reduce_terms(df_frequency, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.loc['total_words'] = df_reduced.sum()\n",
    "df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(df_frequency):\n",
    "    df = df_frequency.copy()\n",
    "    # tf := word frequency / total frequency\n",
    "    df.loc['total_words'] = df.sum()\n",
    "    \n",
    "    df = df.drop('total_words')[:] / df.loc['total_words']\n",
    "    \n",
    "    # idf := log ( len(all_documents) / len(documents_containing_word) )\n",
    "    \n",
    "    corpus_size = df.shape[1]\n",
    "\n",
    "    # number of non-zero cols\n",
    "    df['doc_frequency'] = df.fillna(0).astype(bool).sum(axis=1)\n",
    "        \n",
    "    df['idf'] = np.log( corpus_size / df['doc_frequency'] )\n",
    "    \n",
    "    # tf-idf := tf * idf\n",
    "    _cols = df.columns.difference(['idf', 'doc_frequency'])\n",
    "    df[_cols] = df[_cols].multiply(df[\"idf\"], axis=\"index\")\n",
    "    \n",
    "    df.drop(columns=['doc_frequency', 'idf'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf = get_tf_idf(df_reduced)\n",
    "display(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df_tf_idf.fillna(0).to_numpy()\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
