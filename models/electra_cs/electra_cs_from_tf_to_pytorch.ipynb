{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "electra_cs_from_tf_to_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPp4THPxViy5KZvVTpBHpGv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christee8/lsa-model/blob/master/models/electra_cs/electra_cs_from_tf_to_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWvjqEFUMZ0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "f9c592e2-0234-4509-dbd0-5e4a99199007"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWBtZEdvLNCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "d37c7f07-9e6d-470d-b0a9-5d0539ad3117"
      },
      "source": [
        "!pip3 install --upgrade transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 2.6MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 26.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=5161cce45b4ddc8035b4c273c60c2e7e2b6c47083374c9d03b535aa5887f7a28\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuii18JKM_X8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /gdrive/'My Drive'/question_answering/models/electra_cs/model_base ."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6mAuy-L0tV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "59189609-3149-4ca3-a7f7-77cb0a7ed9d9"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/24)\u001b[K\rremote: Counting objects:   8% (2/24)\u001b[K\rremote: Counting objects:  12% (3/24)\u001b[K\rremote: Counting objects:  16% (4/24)\u001b[K\rremote: Counting objects:  20% (5/24)\u001b[K\rremote: Counting objects:  25% (6/24)\u001b[K\rremote: Counting objects:  29% (7/24)\u001b[K\rremote: Counting objects:  33% (8/24)\u001b[K\rremote: Counting objects:  37% (9/24)\u001b[K\rremote: Counting objects:  41% (10/24)\u001b[K\rremote: Counting objects:  45% (11/24)\u001b[K\rremote: Counting objects:  50% (12/24)\u001b[K\rremote: Counting objects:  54% (13/24)\u001b[K\rremote: Counting objects:  58% (14/24)\u001b[K\rremote: Counting objects:  62% (15/24)\u001b[K\rremote: Counting objects:  66% (16/24)\u001b[K\rremote: Counting objects:  70% (17/24)\u001b[K\rremote: Counting objects:  75% (18/24)\u001b[K\rremote: Counting objects:  79% (19/24)\u001b[K\rremote: Counting objects:  83% (20/24)\u001b[K\rremote: Counting objects:  87% (21/24)\u001b[K\rremote: Counting objects:  91% (22/24)\u001b[K\rremote: Counting objects:  95% (23/24)\u001b[K\rremote: Counting objects: 100% (24/24)\u001b[K\rremote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects:   4% (1/23)\u001b[K\rremote: Compressing objects:   8% (2/23)\u001b[K\rremote: Compressing objects:  13% (3/23)\u001b[K\rremote: Compressing objects:  17% (4/23)\u001b[K\rremote: Compressing objects:  21% (5/23)\u001b[K\rremote: Compressing objects:  26% (6/23)\u001b[K\rremote: Compressing objects:  30% (7/23)\u001b[K\rremote: Compressing objects:  34% (8/23)\u001b[K\rremote: Compressing objects:  39% (9/23)\u001b[K\rremote: Compressing objects:  43% (10/23)\u001b[K\rremote: Compressing objects:  47% (11/23)\u001b[K\rremote: Compressing objects:  52% (12/23)\u001b[K\rremote: Compressing objects:  56% (13/23)\u001b[K\rremote: Compressing objects:  60% (14/23)\u001b[K\rremote: Compressing objects:  65% (15/23)\u001b[K\rremote: Compressing objects:  69% (16/23)\u001b[K\rremote: Compressing objects:  73% (17/23)\u001b[K\rremote: Compressing objects:  78% (18/23)\u001b[K\rremote: Compressing objects:  82% (19/23)\u001b[K\rremote: Compressing objects:  86% (20/23)\u001b[K\rremote: Compressing objects:  91% (21/23)\u001b[K\rremote: Compressing objects:  95% (22/23)\u001b[K\rremote: Compressing objects: 100% (23/23)\u001b[K\rremote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "Receiving objects:   0% (1/37184)   \rReceiving objects:   1% (372/37184)   \rReceiving objects:   2% (744/37184)   \rReceiving objects:   3% (1116/37184)   \rReceiving objects:   4% (1488/37184)   \rReceiving objects:   5% (1860/37184)   \rReceiving objects:   6% (2232/37184)   \rReceiving objects:   7% (2603/37184)   \rReceiving objects:   8% (2975/37184)   \rReceiving objects:   9% (3347/37184)   \rReceiving objects:  10% (3719/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  11% (4091/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  12% (4463/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  13% (4834/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  14% (5206/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  15% (5578/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  16% (5950/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  17% (6322/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  18% (6694/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  19% (7065/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  20% (7437/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  21% (7809/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  21% (8165/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  22% (8181/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  23% (8553/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  24% (8925/37184), 1.23 MiB | 2.29 MiB/s   \rReceiving objects:  25% (9296/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  26% (9668/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  27% (10040/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  28% (10412/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  29% (10784/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  30% (11156/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  31% (11528/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  32% (11899/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  33% (12271/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  34% (12643/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  35% (13015/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  36% (13387/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  37% (13759/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  38% (14130/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  39% (14502/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  40% (14874/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  41% (15246/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  42% (15618/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  43% (15990/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  44% (16361/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  45% (16733/37184), 7.95 MiB | 7.67 MiB/s   \rReceiving objects:  46% (17105/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  47% (17477/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  48% (17849/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  49% (18221/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  50% (18592/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  51% (18964/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  52% (19336/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  53% (19708/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  54% (20080/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  55% (20452/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  56% (20824/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  57% (21195/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  58% (21567/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  58% (21895/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  59% (21939/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  60% (22311/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  61% (22683/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  62% (23055/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  63% (23426/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  64% (23798/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  65% (24170/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  66% (24542/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  67% (24914/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  68% (25286/37184), 14.94 MiB | 9.54 MiB/s   \rReceiving objects:  69% (25657/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  70% (26029/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  71% (26401/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  72% (26773/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  73% (27145/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  74% (27517/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  75% (27888/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  76% (28260/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  77% (28632/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  78% (29004/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  79% (29376/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  80% (29748/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  81% (30120/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  82% (30491/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  83% (30863/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  84% (31235/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  85% (31607/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  86% (31979/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  87% (32351/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  88% (32722/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  89% (33094/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  90% (33466/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  91% (33838/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  92% (34210/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  93% (34582/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  94% (34953/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  95% (35325/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  96% (35697/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  97% (36069/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  98% (36441/37184), 22.90 MiB | 11.09 MiB/s   \rReceiving objects:  99% (36813/37184), 22.90 MiB | 11.09 MiB/s   \rremote: Total 37184 (delta 7), reused 10 (delta 0), pack-reused 37160\u001b[K\n",
            "Receiving objects: 100% (37184/37184), 27.11 MiB | 11.41 MiB/s, done.\n",
            "Resolving deltas: 100% (25720/25720), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-4CUtR9k-t8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7fad3156-7a31-4d11-8199-aa971bab2b62"
      },
      "source": [
        "%env ELECTRA_BASE_DIR=/content/model_base"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: ELECTRA_BASE_DIR=/content/model_base\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9aQvOoUsjWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "309d659d-b7aa-4149-d6c0-bd98b523ce75"
      },
      "source": [
        "!ls $ELECTRA_BASE_DIR"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint_files\t\t\t   electra_cs_base_vocab.txt\n",
            "electra_cs_base_discriminator.bin\t   graph.pbtxt\n",
            "electra_cs_base_discriminator_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vc7SIkzZYXf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55252f28-5812-47c9-dc06-d7bb07014831"
      },
      "source": [
        "# https://github.com/huggingface/transformers/blob/master/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py\n",
        "\n",
        "!python3 ./transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py \\\n",
        "--tf_checkpoint_path $ELECTRA_BASE_DIR/checkpoint_files/model.ckpt-847000 \\\n",
        "--pytorch_dump_path $ELECTRA_BASE_DIR/electra_cs_base_discriminator.bin \\\n",
        "--discriminator_or_generator 'discriminator' \\\n",
        "--config_file $ELECTRA_BASE_DIR/electra_cs_base_discriminator_config.json"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-10 08:56:37.802190: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Building PyTorch model from configuration: ElectraConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 127847\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_electra:Converting TensorFlow checkpoint from /content/model_base/checkpoint_files/model.ckpt-847000\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense_1/bias with shape [1]\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense_1/bias/adam_m with shape [1]\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense_1/bias/adam_v with shape [1]\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense_1/kernel with shape [768, 1]\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense_1/kernel/adam_m with shape [768, 1]\n",
            "INFO:transformers.modeling_electra:Loading TF weight discriminator_predictions/dense_1/kernel/adam_v with shape [768, 1]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/position_embeddings with shape [512, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/position_embeddings/adam_m with shape [512, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/position_embeddings/adam_v with shape [512, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/token_type_embeddings with shape [2, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/token_type_embeddings/adam_m with shape [2, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/token_type_embeddings/adam_v with shape [2, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/word_embeddings with shape [127847, 768]\n",
            "2020-08-10 08:56:47.404775: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 392745984 exceeds 10% of free system memory.\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/word_embeddings/adam_m with shape [127847, 768]\n",
            "2020-08-10 08:56:55.021690: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 392745984 exceeds 10% of free system memory.\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/embeddings/word_embeddings/adam_v with shape [127847, 768]\n",
            "2020-08-10 08:57:02.567350: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 392745984 exceeds 10% of free system memory.\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/kernel/adam_m with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/kernel/adam_v with shape [768, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/bias/adam_m with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/bias/adam_v with shape [3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [768, 3072]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/kernel/adam_m with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/kernel/adam_v with shape [3072, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/embeddings_project/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/embeddings_project/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/embeddings_project/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/embeddings_project/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/embeddings_project/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/embeddings_project/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/kernel with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/kernel/adam_m with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/kernel/adam_v with shape [192, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/beta with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/gamma with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/bias with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/bias/adam_m with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/bias/adam_v with shape [192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/kernel with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/kernel/adam_m with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/kernel/adam_v with shape [768, 192]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/LayerNorm/beta/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/LayerNorm/beta/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/LayerNorm/gamma/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/LayerNorm/gamma/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/dense/bias/adam_m with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/dense/bias/adam_v with shape [768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/dense/kernel with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/dense/kernel/adam_m with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/dense/kernel/adam_v with shape [192, 768]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/output_bias with shape [127847]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/output_bias/adam_m with shape [127847]\n",
            "INFO:transformers.modeling_electra:Loading TF weight generator_predictions/output_bias/adam_v with shape [127847]\n",
            "INFO:transformers.modeling_electra:Loading TF weight global_step with shape []\n",
            "Initialize PyTorch weight ['discriminator_predictions', 'dense', 'bias'] discriminator_predictions/dense/bias\n",
            "Skipping discriminator_predictions/dense/bias/adam_m ['discriminator_predictions', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping discriminator_predictions/dense/bias/adam_v ['discriminator_predictions', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['discriminator_predictions', 'dense', 'kernel'] discriminator_predictions/dense/kernel\n",
            "Skipping discriminator_predictions/dense/kernel/adam_m ['discriminator_predictions', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping discriminator_predictions/dense/kernel/adam_v ['discriminator_predictions', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['discriminator_predictions', 'dense_prediction', 'bias'] discriminator_predictions/dense_1/bias\n",
            "Skipping discriminator_predictions/dense_1/bias/adam_m ['discriminator_predictions', 'dense_prediction', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping discriminator_predictions/dense_1/bias/adam_v ['discriminator_predictions', 'dense_prediction', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['discriminator_predictions', 'dense_prediction', 'kernel'] discriminator_predictions/dense_1/kernel\n",
            "Skipping discriminator_predictions/dense_1/kernel/adam_m ['discriminator_predictions', 'dense_prediction', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping discriminator_predictions/dense_1/kernel/adam_v ['discriminator_predictions', 'dense_prediction', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'embeddings', 'LayerNorm', 'beta'] electra/embeddings/LayerNorm/beta\n",
            "Skipping electra/embeddings/LayerNorm/beta/adam_m ['electra', 'embeddings', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/embeddings/LayerNorm/beta/adam_v ['electra', 'embeddings', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'embeddings', 'LayerNorm', 'gamma'] electra/embeddings/LayerNorm/gamma\n",
            "Skipping electra/embeddings/LayerNorm/gamma/adam_m ['electra', 'embeddings', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/embeddings/LayerNorm/gamma/adam_v ['electra', 'embeddings', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'embeddings', 'position_embeddings'] electra/embeddings/position_embeddings\n",
            "Skipping electra/embeddings/position_embeddings/adam_m ['electra', 'embeddings', 'position_embeddings', 'adam_m'] 'Embedding' object has no attribute 'adam_m'\n",
            "Skipping electra/embeddings/position_embeddings/adam_v ['electra', 'embeddings', 'position_embeddings', 'adam_v'] 'Embedding' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'embeddings', 'token_type_embeddings'] electra/embeddings/token_type_embeddings\n",
            "Skipping electra/embeddings/token_type_embeddings/adam_m ['electra', 'embeddings', 'token_type_embeddings', 'adam_m'] 'Embedding' object has no attribute 'adam_m'\n",
            "Skipping electra/embeddings/token_type_embeddings/adam_v ['electra', 'embeddings', 'token_type_embeddings', 'adam_v'] 'Embedding' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'embeddings', 'word_embeddings'] electra/embeddings/word_embeddings\n",
            "Skipping electra/embeddings/word_embeddings/adam_m ['electra', 'embeddings', 'word_embeddings', 'adam_m'] 'Embedding' object has no attribute 'adam_m'\n",
            "Skipping electra/embeddings/word_embeddings/adam_v ['electra', 'embeddings', 'word_embeddings', 'adam_v'] 'Embedding' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_0/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_0/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_0/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_0/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_0/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_0/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_0/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_0/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_0/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_0/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_0/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_0/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_0/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_0/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_0/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_0/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_0/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_0/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias'] electra/encoder/layer_0/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_0/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_0/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_0/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_0/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_0/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_0/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_0/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'dense', 'bias'] electra/encoder/layer_0/output/dense/bias\n",
            "Skipping electra/encoder/layer_0/output/dense/bias/adam_m ['electra', 'encoder', 'layer_0', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/output/dense/bias/adam_v ['electra', 'encoder', 'layer_0', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'dense', 'kernel'] electra/encoder/layer_0/output/dense/kernel\n",
            "Skipping electra/encoder/layer_0/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_0', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_0/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_0', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_1/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_1/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_1/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_1/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_1/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_1/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_1/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_1/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_1/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_1/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_1/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_1/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_1/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_1/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_1/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_1/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_1/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_1/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias'] electra/encoder/layer_1/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_1/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_1/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_1/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_1/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_1/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_1/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_1/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'dense', 'bias'] electra/encoder/layer_1/output/dense/bias\n",
            "Skipping electra/encoder/layer_1/output/dense/bias/adam_m ['electra', 'encoder', 'layer_1', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/output/dense/bias/adam_v ['electra', 'encoder', 'layer_1', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'dense', 'kernel'] electra/encoder/layer_1/output/dense/kernel\n",
            "Skipping electra/encoder/layer_1/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_1', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_1/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_1', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_10/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_10/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_10/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_10/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_10/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_10/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_10/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_10/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_10/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_10/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_10/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_10/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_10/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_10/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_10/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_10/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_10/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_10/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias'] electra/encoder/layer_10/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_10/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_10/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_10/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_10/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_10/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_10/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_10/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'dense', 'bias'] electra/encoder/layer_10/output/dense/bias\n",
            "Skipping electra/encoder/layer_10/output/dense/bias/adam_m ['electra', 'encoder', 'layer_10', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/output/dense/bias/adam_v ['electra', 'encoder', 'layer_10', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'dense', 'kernel'] electra/encoder/layer_10/output/dense/kernel\n",
            "Skipping electra/encoder/layer_10/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_10', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_10/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_10', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_11/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_11/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_11/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_11/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_11/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_11/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_11/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_11/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_11/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_11/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_11/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_11/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_11/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_11/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_11/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_11/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_11/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_11/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias'] electra/encoder/layer_11/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_11/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_11/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_11/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_11/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_11/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_11/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_11/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'dense', 'bias'] electra/encoder/layer_11/output/dense/bias\n",
            "Skipping electra/encoder/layer_11/output/dense/bias/adam_m ['electra', 'encoder', 'layer_11', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/output/dense/bias/adam_v ['electra', 'encoder', 'layer_11', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'dense', 'kernel'] electra/encoder/layer_11/output/dense/kernel\n",
            "Skipping electra/encoder/layer_11/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_11', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_11/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_11', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_2/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_2/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_2/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_2/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_2/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_2/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_2/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_2/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_2/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_2/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_2/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_2/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_2/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_2/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_2/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_2/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_2/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_2/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias'] electra/encoder/layer_2/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_2/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_2/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_2/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_2/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_2/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_2/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_2/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'dense', 'bias'] electra/encoder/layer_2/output/dense/bias\n",
            "Skipping electra/encoder/layer_2/output/dense/bias/adam_m ['electra', 'encoder', 'layer_2', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/output/dense/bias/adam_v ['electra', 'encoder', 'layer_2', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'dense', 'kernel'] electra/encoder/layer_2/output/dense/kernel\n",
            "Skipping electra/encoder/layer_2/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_2', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_2/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_2', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_3/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_3/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_3/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_3/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_3/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_3/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_3/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_3/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_3/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_3/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_3/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_3/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_3/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_3/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_3/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_3/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_3/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_3/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias'] electra/encoder/layer_3/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_3/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_3/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_3/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_3/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_3/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_3/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_3/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'dense', 'bias'] electra/encoder/layer_3/output/dense/bias\n",
            "Skipping electra/encoder/layer_3/output/dense/bias/adam_m ['electra', 'encoder', 'layer_3', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/output/dense/bias/adam_v ['electra', 'encoder', 'layer_3', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'dense', 'kernel'] electra/encoder/layer_3/output/dense/kernel\n",
            "Skipping electra/encoder/layer_3/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_3', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_3/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_3', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_4/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_4/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_4/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_4/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_4/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_4/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_4/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_4/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_4/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_4/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_4/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_4/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_4/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_4/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_4/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_4/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_4/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_4/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias'] electra/encoder/layer_4/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_4/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_4/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_4/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_4/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_4/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_4/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_4/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'dense', 'bias'] electra/encoder/layer_4/output/dense/bias\n",
            "Skipping electra/encoder/layer_4/output/dense/bias/adam_m ['electra', 'encoder', 'layer_4', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/output/dense/bias/adam_v ['electra', 'encoder', 'layer_4', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'dense', 'kernel'] electra/encoder/layer_4/output/dense/kernel\n",
            "Skipping electra/encoder/layer_4/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_4', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_4/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_4', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_5/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_5/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_5/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_5/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_5/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_5/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_5/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_5/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_5/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_5/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_5/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_5/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_5/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_5/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_5/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_5/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_5/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_5/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias'] electra/encoder/layer_5/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_5/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_5/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_5/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_5/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_5/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_5/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_5/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'dense', 'bias'] electra/encoder/layer_5/output/dense/bias\n",
            "Skipping electra/encoder/layer_5/output/dense/bias/adam_m ['electra', 'encoder', 'layer_5', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/output/dense/bias/adam_v ['electra', 'encoder', 'layer_5', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'dense', 'kernel'] electra/encoder/layer_5/output/dense/kernel\n",
            "Skipping electra/encoder/layer_5/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_5', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_5/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_5', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_6/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_6/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_6/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_6/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_6/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_6/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_6/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_6/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_6/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_6/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_6/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_6/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_6/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_6/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_6/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_6/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_6/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_6/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias'] electra/encoder/layer_6/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_6/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_6/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_6/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_6/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_6/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_6/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_6/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'dense', 'bias'] electra/encoder/layer_6/output/dense/bias\n",
            "Skipping electra/encoder/layer_6/output/dense/bias/adam_m ['electra', 'encoder', 'layer_6', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/output/dense/bias/adam_v ['electra', 'encoder', 'layer_6', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'dense', 'kernel'] electra/encoder/layer_6/output/dense/kernel\n",
            "Skipping electra/encoder/layer_6/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_6', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_6/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_6', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_7/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_7/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_7/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_7/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_7/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_7/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_7/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_7/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_7/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_7/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_7/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_7/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_7/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_7/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_7/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_7/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_7/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_7/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias'] electra/encoder/layer_7/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_7/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_7/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_7/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_7/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_7/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_7/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_7/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'dense', 'bias'] electra/encoder/layer_7/output/dense/bias\n",
            "Skipping electra/encoder/layer_7/output/dense/bias/adam_m ['electra', 'encoder', 'layer_7', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/output/dense/bias/adam_v ['electra', 'encoder', 'layer_7', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'dense', 'kernel'] electra/encoder/layer_7/output/dense/kernel\n",
            "Skipping electra/encoder/layer_7/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_7', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_7/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_7', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_8/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_8/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_8/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_8/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_8/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_8/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_8/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_8/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_8/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_8/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_8/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_8/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_8/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_8/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_8/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_8/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_8/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_8/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias'] electra/encoder/layer_8/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_8/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_8/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_8/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_8/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_8/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_8/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_8/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'dense', 'bias'] electra/encoder/layer_8/output/dense/bias\n",
            "Skipping electra/encoder/layer_8/output/dense/bias/adam_m ['electra', 'encoder', 'layer_8', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/output/dense/bias/adam_v ['electra', 'encoder', 'layer_8', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'dense', 'kernel'] electra/encoder/layer_8/output/dense/kernel\n",
            "Skipping electra/encoder/layer_8/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_8', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_8/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_8', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_9/attention/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_9/attention/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_9/attention/output/dense/bias\n",
            "Skipping electra/encoder/layer_9/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_9/attention/output/dense/kernel\n",
            "Skipping electra/encoder/layer_9/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_9/attention/self/key/bias\n",
            "Skipping electra/encoder/layer_9/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_9/attention/self/key/kernel\n",
            "Skipping electra/encoder/layer_9/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_9/attention/self/query/bias\n",
            "Skipping electra/encoder/layer_9/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_9/attention/self/query/kernel\n",
            "Skipping electra/encoder/layer_9/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_9/attention/self/value/bias\n",
            "Skipping electra/encoder/layer_9/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_9/attention/self/value/kernel\n",
            "Skipping electra/encoder/layer_9/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias'] electra/encoder/layer_9/intermediate/dense/bias\n",
            "Skipping electra/encoder/layer_9/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_9/intermediate/dense/kernel\n",
            "Skipping electra/encoder/layer_9/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_9/output/LayerNorm/beta\n",
            "Skipping electra/encoder/layer_9/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_9/output/LayerNorm/gamma\n",
            "Skipping electra/encoder/layer_9/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'dense', 'bias'] electra/encoder/layer_9/output/dense/bias\n",
            "Skipping electra/encoder/layer_9/output/dense/bias/adam_m ['electra', 'encoder', 'layer_9', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/output/dense/bias/adam_v ['electra', 'encoder', 'layer_9', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'dense', 'kernel'] electra/encoder/layer_9/output/dense/kernel\n",
            "Skipping electra/encoder/layer_9/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_9', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
            "Skipping electra/encoder/layer_9/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_9', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
            "Skipping generator/embeddings_project/bias ['generator', 'embeddings_project', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/embeddings_project/bias/adam_m ['generator', 'embeddings_project', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/embeddings_project/bias/adam_v ['generator', 'embeddings_project', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/embeddings_project/kernel ['generator', 'embeddings_project', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/embeddings_project/kernel/adam_m ['generator', 'embeddings_project', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/embeddings_project/kernel/adam_v ['generator', 'embeddings_project', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/dense/bias ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/dense/kernel ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/key/bias ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/key/kernel ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/query/bias ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/query/kernel ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/value/bias ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/value/kernel ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/intermediate/dense/bias ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/intermediate/dense/kernel ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/LayerNorm/beta ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/LayerNorm/gamma ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/dense/bias ['generator', 'encoder', 'layer_0', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/dense/bias/adam_m ['generator', 'encoder', 'layer_0', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/dense/bias/adam_v ['generator', 'encoder', 'layer_0', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/dense/kernel ['generator', 'encoder', 'layer_0', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_0', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_0/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_0', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/dense/bias ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/dense/kernel ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/key/bias ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/key/kernel ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/query/bias ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/query/kernel ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/value/bias ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/value/kernel ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/intermediate/dense/bias ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/intermediate/dense/kernel ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/LayerNorm/beta ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/LayerNorm/gamma ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/dense/bias ['generator', 'encoder', 'layer_1', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/dense/bias/adam_m ['generator', 'encoder', 'layer_1', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/dense/bias/adam_v ['generator', 'encoder', 'layer_1', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/dense/kernel ['generator', 'encoder', 'layer_1', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_1', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_1/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_1', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/dense/bias ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/dense/kernel ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/key/bias ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/key/kernel ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/query/bias ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/query/kernel ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/value/bias ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/value/kernel ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/intermediate/dense/bias ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/intermediate/dense/kernel ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/LayerNorm/beta ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/LayerNorm/gamma ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/dense/bias ['generator', 'encoder', 'layer_10', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/dense/bias/adam_m ['generator', 'encoder', 'layer_10', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/dense/bias/adam_v ['generator', 'encoder', 'layer_10', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/dense/kernel ['generator', 'encoder', 'layer_10', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_10', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_10/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_10', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/dense/bias ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/dense/kernel ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/key/bias ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/key/kernel ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/query/bias ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/query/kernel ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/value/bias ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/value/kernel ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/intermediate/dense/bias ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/intermediate/dense/kernel ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/LayerNorm/beta ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/LayerNorm/gamma ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/dense/bias ['generator', 'encoder', 'layer_11', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/dense/bias/adam_m ['generator', 'encoder', 'layer_11', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/dense/bias/adam_v ['generator', 'encoder', 'layer_11', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/dense/kernel ['generator', 'encoder', 'layer_11', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_11', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_11/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_11', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/dense/bias ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/dense/kernel ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/key/bias ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/key/kernel ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/query/bias ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/query/kernel ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/value/bias ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/value/kernel ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/intermediate/dense/bias ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/intermediate/dense/kernel ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/LayerNorm/beta ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/LayerNorm/gamma ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/dense/bias ['generator', 'encoder', 'layer_2', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/dense/bias/adam_m ['generator', 'encoder', 'layer_2', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/dense/bias/adam_v ['generator', 'encoder', 'layer_2', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/dense/kernel ['generator', 'encoder', 'layer_2', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_2', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_2/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_2', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/dense/bias ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/dense/kernel ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/key/bias ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/key/kernel ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/query/bias ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/query/kernel ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/value/bias ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/value/kernel ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/intermediate/dense/bias ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/intermediate/dense/kernel ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/LayerNorm/beta ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/LayerNorm/gamma ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/dense/bias ['generator', 'encoder', 'layer_3', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/dense/bias/adam_m ['generator', 'encoder', 'layer_3', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/dense/bias/adam_v ['generator', 'encoder', 'layer_3', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/dense/kernel ['generator', 'encoder', 'layer_3', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_3', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_3/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_3', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/dense/bias ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/dense/kernel ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/key/bias ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/key/kernel ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/query/bias ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/query/kernel ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/value/bias ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/value/kernel ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/intermediate/dense/bias ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/intermediate/dense/kernel ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/LayerNorm/beta ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/LayerNorm/gamma ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/dense/bias ['generator', 'encoder', 'layer_4', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/dense/bias/adam_m ['generator', 'encoder', 'layer_4', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/dense/bias/adam_v ['generator', 'encoder', 'layer_4', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/dense/kernel ['generator', 'encoder', 'layer_4', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_4', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_4/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_4', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/dense/bias ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/dense/kernel ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/key/bias ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/key/kernel ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/query/bias ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/query/kernel ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/value/bias ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/value/kernel ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/intermediate/dense/bias ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/intermediate/dense/kernel ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/LayerNorm/beta ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/LayerNorm/gamma ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/dense/bias ['generator', 'encoder', 'layer_5', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/dense/bias/adam_m ['generator', 'encoder', 'layer_5', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/dense/bias/adam_v ['generator', 'encoder', 'layer_5', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/dense/kernel ['generator', 'encoder', 'layer_5', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_5', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_5/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_5', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/dense/bias ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/dense/kernel ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/key/bias ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/key/kernel ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/query/bias ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/query/kernel ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/value/bias ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/value/kernel ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/intermediate/dense/bias ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/intermediate/dense/kernel ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/LayerNorm/beta ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/LayerNorm/gamma ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/dense/bias ['generator', 'encoder', 'layer_6', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/dense/bias/adam_m ['generator', 'encoder', 'layer_6', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/dense/bias/adam_v ['generator', 'encoder', 'layer_6', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/dense/kernel ['generator', 'encoder', 'layer_6', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_6', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_6/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_6', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/dense/bias ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/dense/kernel ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/key/bias ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/key/kernel ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/query/bias ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/query/kernel ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/value/bias ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/value/kernel ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/intermediate/dense/bias ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/intermediate/dense/kernel ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/LayerNorm/beta ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/LayerNorm/gamma ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/dense/bias ['generator', 'encoder', 'layer_7', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/dense/bias/adam_m ['generator', 'encoder', 'layer_7', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/dense/bias/adam_v ['generator', 'encoder', 'layer_7', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/dense/kernel ['generator', 'encoder', 'layer_7', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_7', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_7/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_7', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/dense/bias ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/dense/kernel ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/key/bias ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/key/kernel ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/query/bias ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/query/kernel ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/value/bias ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/value/kernel ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/intermediate/dense/bias ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/intermediate/dense/kernel ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/LayerNorm/beta ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/LayerNorm/gamma ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/dense/bias ['generator', 'encoder', 'layer_8', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/dense/bias/adam_m ['generator', 'encoder', 'layer_8', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/dense/bias/adam_v ['generator', 'encoder', 'layer_8', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/dense/kernel ['generator', 'encoder', 'layer_8', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_8', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_8/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_8', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/dense/bias ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/dense/kernel ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/key/bias ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/key/kernel ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/query/bias ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/query/kernel ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/value/bias ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/value/kernel ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/intermediate/dense/bias ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/intermediate/dense/kernel ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/LayerNorm/beta ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/LayerNorm/gamma ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/dense/bias ['generator', 'encoder', 'layer_9', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/dense/bias/adam_m ['generator', 'encoder', 'layer_9', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/dense/bias/adam_v ['generator', 'encoder', 'layer_9', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/dense/kernel ['generator', 'encoder', 'layer_9', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_9', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator/encoder/layer_9/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_9', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
            "Skipping generator_predictions/LayerNorm/beta ['generator_predictions', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/LayerNorm/beta/adam_m ['generator_predictions', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/LayerNorm/beta/adam_v ['generator_predictions', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/LayerNorm/gamma ['generator_predictions', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/LayerNorm/gamma/adam_m ['generator_predictions', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/LayerNorm/gamma/adam_v ['generator_predictions', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/dense/bias ['generator_predictions', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/dense/bias/adam_m ['generator_predictions', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/dense/bias/adam_v ['generator_predictions', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/dense/kernel ['generator_predictions', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/dense/kernel/adam_m ['generator_predictions', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/dense/kernel/adam_v ['generator_predictions', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
            "Skipping generator_predictions/output_bias ['generator_lm_head', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator_lm_head'\n",
            "Skipping generator_predictions/output_bias/adam_m ['generator_lm_head', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator_lm_head'\n",
            "Skipping generator_predictions/output_bias/adam_v ['generator_lm_head', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator_lm_head'\n",
            "INFO:transformers.modeling_electra:Skipping global_step\n",
            "Save PyTorch model to /content/model_base/electra_cs_base_discriminator.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJEX0ahT6Qci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp $ELECTRA_BASE_DIR/electra_cs_base_discriminator.bin /gdrive/'My Drive'/question_answering/models/electra_cs/model_base/electra_cs_base_discriminator.bin"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gN__AUbZlYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# API, doesn't work for Electra\n",
        "# https://medium.com/analytics-vidhya/this-article-shows-how-to-convert-the-tensorflow-model-to-the-huggingface-transformers-model-4a39a09a2242\n",
        "\n",
        "# !transformers-cli convert \\\n",
        "# --model_type bert \\\n",
        "# --tf_checkpoint $ELECTRA_BASE_DIR/checkpoint_files/model.ckpt-847000 \\\n",
        "# --pytorch_dump_output $ELECTRA_BASE_DIR/electra_cs_base_discriminator.bin \\\n",
        "# --config $ELECTRA_BASE_DIR/electra_cs_discriminator_config.json \\"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NIUXgwjboP6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dde2ecc0-a6af-4d25-cfa4-0a2ba5b4c62e"
      },
      "source": [
        "from transformers import ElectraForPreTraining, ElectraConfig, ElectraTokenizer\n",
        "import torch\n",
        "\n",
        "# https://huggingface.co/transformers/model_doc/electra.html#electraconfig\n",
        "configuration = ElectraConfig.from_json_file('model_base/electra_cs_base_discriminator_config.json')\n",
        "discriminator = ElectraForPreTraining.from_pretrained('model_base/electra_cs_base_discriminator.bin', config=configuration)\n",
        "tokenizer = ElectraTokenizer(\"model_base/electra_cs_base_vocab.txt\", do_lower_case=False)\n",
        "discriminator.eval()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElectraForPreTraining(\n",
              "  (electra): ElectraModel(\n",
              "    (embeddings): ElectraEmbeddings(\n",
              "      (word_embeddings): Embedding(127847, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (discriminator_predictions): ElectraDiscriminatorPredictions(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dense_prediction): Linear(in_features=768, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEG-YuGRcPuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_electra_discriminator(fake_sentence, discriminator, tokenizer):\n",
        "  fake_tokens = tokenizer.tokenize(fake_sentence)\n",
        "  fake_inputs = tokenizer.encode(fake_sentence, return_tensors=\"pt\")\n",
        "  \n",
        "  discriminator_outputs = discriminator(fake_inputs)\n",
        "  predictions = torch.round((torch.sign(discriminator_outputs[0]) + 1) / 2)\n",
        "\n",
        "  [print(\"%7s\" % token, end=\"\") for token in fake_tokens]\n",
        "\n",
        "  [print(\"%7s\" % prediction, end=\"\") for prediction in predictions.tolist()]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRj6vBnkklO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7869cb73-e647-4852-f62a-d1880f86ea4c"
      },
      "source": [
        "fake_sentence = \"Rychlá hnědá liška chyba přes líného psa\"\n",
        "test_electra_discriminator(fake_sentence=fake_sentence, discriminator=discriminator, tokenizer=tokenizer)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Rychl    ##á  hnědá  liška  chyba   přes   líné   ##ho    psa    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXJjM1zIks2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f85f2d0d-1590-4653-c567-48be2bb0c827"
      },
      "source": [
        "fake_sentence = \"Dnes jsem šla do chyba a koupila si zmrzlinu\"\n",
        "test_electra_discriminator(fake_sentence=fake_sentence, discriminator=discriminator, tokenizer=tokenizer)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Dnes   jsem    šla     do  chyba      a  koupi   ##la     si  zmrzl  ##inu    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skI3tH5tmWrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}