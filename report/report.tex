%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Assignment
% LaTeX Template
% Version 2.0 (12/1/2019)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{scrartcl} % Font size

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{	
	\normalfont\normalsize
	\textsc{ČVUT, Fakulta informačních technologií}\\ % Your university, school and/or department name(s)
	\textsc{Vyhledávání na webu a v multimediálních databázích}\\
	\textsc{Letní semestr 2019/2020}\\
	\textsc{Závěrečná zpráva k projektu}\\
	\vspace{25pt} % Whitespace
	\rule{\linewidth}{0.5pt}\\ % Thin top horizontal rule
	\vspace{20pt} % Whitespace
	{\huge LSI vektorový model}\\ % The assignment title
	\vspace{12pt} % Whitespace
	\rule{\linewidth}{2pt}\\ % Thick bottom horizontal rule
	\vspace{12pt} % Whitespace
}

\author{\LARGE David Mašek a Kristýna Klesnilová} % Your name
\date{\normalsize\today} % Today's date (\today) or a custom date

\begin{document}

\maketitle % Print the title

\newpage

\tableofcontents

\newpage

\section{Popis projektu}

V tomto projektu implementujeme \emph{LSI vektorový model} sloužící k podobnostnímu vyhledávání v databázi anglických textových dokumentů. Tuto funkcionalitu následně vizualizujeme pomocí webového interface, který uživateli umožňuje procházet databázi článků na základě doporučování nejpodobnějších článků k právě čtenému.

\bigskip 

V experimentální části projektu jsme se dále zaměřili na:
\begin{itemize}
	\item Určení optimálního počtu konceptů
	\item Porovnání vlivu LSI na kvalitu výsledků vyhledávání s ohledem na výskyt synonym a homonym
	\item Porovnání průchodu pomocí LSI vektorového modelu se sekvenčním průchodem databáze s ohledem na čas vykonání dotazu
	\item Vliv různých vnitřních parametrů na výkon algoritmu (změna počtu konceptů, změna počtu extrahovaných termů, použití lemmatizace namísto stemmingu, odstranění číslovek při preprocesingu, použití jiného vzorce pro výpočet vah termů)
\end{itemize}

\bigskip 

Celý náš projekt je volně dostupný k vyzkoušení na: \url{https://bi-vwm-lsi-demo.herokuapp.com/}.

\section{Způsob řešení}

\subsection{Preprocesing dokumentů}

Jako první v naší aplikaci začínáme s preprocesingem dokumentů. Slova z jednotlivých dokumentů převedeme na malá písmena a odstraníme z nich nevýznamová slova a interpunkci. K identifikaci nevýznamových slov používáme seznam anglických nevýznamových slov. Jako parametr programu posíláme také, zda má z dokumentů odstranit i číslovky. Následně na zbylé termy aplikujeme \emph{stemming} či \emph{lemmatizaci}. Tím se snažíme slova, která mají stejný slovní základ, vyjádřit pouze jedním termem. Stemming to dělá pomocí algoritmu, kterým odsekává přípony a koncovky slova. Lemmatizace na to jde o něco chytřeji, podle kontextu slova se pokusí určit, o jaký slovní druh se jedná, a podle toho ho zkrátit.\footnote{\url{nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html}} Porovnání jejich použití v programu se dále věnujeme v experimentální části.

\subsection{Výpočet vah termů}\label{subsec:term_weights}
V aplikaci vytváříme matici $M_w$, která má v řádcích jednotlivé termy a ve sloupcích jejich váhy v jednotlivých dokumentech.

\bigskip 

Začneme tím, že si vytvoříme matici počtu výskytů jednotlivých termů v jednotlivých dokumentech. Počet termů v této matici poté dále zredukujeme, abychom pracovali jen s těmi nejdůležitějšími. Funkci pro redukci termů posíláme následující parametry:
\begin{itemize}
	\item \emph{max\_df} - termy nacházející se ve více \% dokumentů, než udává číslo 100 * \emph{max\_df}, z matice odstraníme
	\item \emph{min\_df} - termy nacházející se v méně nebo stejně dokumentech, než udává číslo \emph{min\_df}, z matice odstraníme
	\item \emph{max\_terms} - maximální počet termů, které si v aplikaci necháme
	\item \emph{keep\_less\_freq} - udává, zda si při výběru \emph{max\_terms} termů nechat ty nejméně či nejvíce často zastoupené v dokumentech  
\end{itemize}

\bigskip

Z této zredukované matice poté již spočteme matici $M_w$. Pro výpočet vah jednotlivých termů používáme metodiku \emph{tf-idf}. Váhu termu $t_i$ v dokumentu $d_j$ spočítáme podle vzorce:

\begin{align}
	\begin{split}
		\mathit{w_{ij} = tf_{ij} \cdot idf_{ij}}
	\end{split}					
\end{align}

kde $\mathit{tf_{ij}}$ reprezentuje normalizovanou četnost termu $t_i$ v dokumentu $d_j$ a spočítame ji podle vzorce\footnote{\url{https://en.wikipedia.org/wiki/Tf\%E2\%80\%93idf}} :

\begin{align}
	\begin{split}
		\mathit{tf_{ij} = \frac{f_{ij}}{nt_j}}
	\end{split}					
\end{align}

kde $\mathit{f_{ij}}$ je četnost výskytu termu $t_i$ v dokumentu $d_j$, kterou normalizujeme číslem $\mathit{nt_j}$ vyjadřujícím celkový počet termů v dokumentu $d_j$. V přednášových slidech je použita normalizace jiná, $\mathit{tf_{ij}}$ se tam počítá podle vzorce:

\begin{align}
	\begin{split}
		\mathit{tf_{ij} = \frac{f_{ij}}{max_i\{f_{ij}\}}}
	\end{split}					
\end{align}

kde $\mathit{max_i\{f_{ij}\}}$ vrací nejvyšší četnost termu $t_i$ přes celou kolekci dokumentů. Tento způsob normalizace nám vrací spíše horší výsledky, jejich porovnáním se zabýváme v experimentální části.

$\mathit{idf_{ij}}$ reprezentuje převrácenou četnost $t_i$ ve všech dokumentech a spočítá se podle vzorce:

\begin{align}
	\begin{split}
		\mathit{idf_{ij} = \log_2 (\frac{n}{df_i})}
	\end{split}					
\end{align}

kde $n$ je celkový počet dokumentů a $\mathit{df_i}$ reprezentuje celkový počet dokumentů obsahujících term $t_i$.

\subsection{Implementace LSI}

Jakmile máme vytvořenou matici vah termů $M_w$, můžeme přistoupit k samotné implementaci LSI. Princip LSI spočívá v tom, že s pomocí \emph{singulárního rozkladu (SVD)} seskupíme tematicky podobné články do jednotlivých $k$ konceptů. Vlivem počtu konceptů na kvalitu výsledků se dále zabýváme v experimentální sekci.

\bigskip 

Singulární rozklad nám matici $M_w$ rozloží následovně:

\begin{align}
	\begin{split}
		M_w = U \cdot S \cdot V^T
	\end{split}					
\label{eq:svd}\
\end{align}

kde řádky matice $U$ jsou obrazy řádků matice $M_w$, sloupce matice $V$ jsou obrazy sloupců matice $M_w$ a matice $S$ obsahuje na diagonále \emph{singulární hodnoty (absolutní hodnoty vlastních čísel)} matice $M_w$ v sestupném pořadí. Z těchto matic získáme \emph{concept-by-document} matici $M_{cd}$ jako:

\begin{align}
	\begin{split}
		M_{cd} = S[k,k] \cdot V^T[k,:]
	\end{split}					
\end{align}

kde $S[k,k]$ značí prvních $k$ řádků a sloupců matice $S$ a $V^T[k,:]$ značí prvních $k$ řádků matice $V^T$, kde $k$ je počet konceptů. Nenásobíme tedy celou maticí $M_{cd}$, ale pouze její část podle počtu konceptů.

\bigskip

Matici projekce dotazu do prostoru konceptů $M_q$ pak získáme jako:

\begin{align}
	\begin{split}
		M_q = U^T[k,:]
	\end{split}					
\end{align}

kde $U^T[k,:]$ značí prvních $k$ řádků matice $U^T$.

\subsection{Vyhodnocení dotazu}

Při dotazu na nejpodobnější dokumenty k dokumentu $d_j$ převedeme dotaz do prostoru konceptů na vektor $V_c$ pomocí vzorce:

\begin{align}
	\begin{split}
		V_c = M_q \cdot M_{w_{:,j}}
	\end{split}					
\end{align}

kde $M_{w_{:,j}}$ značí j-tý sloupec matice $M_w$.

\bigskip

Vektor $V_c$ poté pomocí \emph{kosinové podobnosti} porovnáme se sloupcovými vektory matice $M_{cd}$. Indexy nejpodobnějších sloupcových vektorů matice $M_{cd}$ pak vrátíme jako indexy nejpodobnějších dokumentů k dokumentu dotazu $d_j$. Spolu s indexy vrátíme i samotnou hodnotu kosinové podobnosti.

\section{Implementace}

Celý projekt jsme programovali v jazyce \emph{Python}. Práci nám velmi usnadnila jeho knihovna \emph{NLTK}\footnote{\url{https://www.nltk.org/}} nabízející rozsáhlou funkcionalitu pro práci s přirozeným jazykem. Využili jsme například \emph{WordNetLemmatizer} pro lemmatizaci či \emph{SnowballStemmer} pro stemming. Dále jsme v programu hojně využívali Python knihovny \emph{pandas}\footnote{\url{https://pandas.pydata.org/}} a \emph{numpy}\footnote{\url{https://numpy.org/}}.

\bigskip

Ukládání dat v projektu řešíme přes CSV soubory, ke kterým přistupujeme přes pandas funkce. V jednom souboru máme uložený dataset nad kterým provádíme LSI. V dalších souborech pak máme uložené matice $M_w$, $M_{cd}$ a $M_q$, abychom je mohli cachovat a přepočítavat jen při změně LSI parametrů, které máme uložené v souboru \emph{server/lsa\_config.json}.

\bigskip

Procházení článků vizualizujeme v prohlížeči pomocí \emph{Flask}\footnote{\url{https://flask.palletsprojects.com/en/1.1.x/}} web serveru. Jako dataset v našem projektu používáme anglicky psané novinové články stažené z \emph{kaggle.com}\footnote{\url{https://www.kaggle.com/snapcrack/all-the-news}}. Dataset nepoužíváme celý, vybrali jsme z něj pouze 996 článků.

\section{Příklad výstupu}

\begin{figure}[h] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.9\columnwidth]{images/output.png}
	\caption{Příklad výstupu aplikace}
	\label{fig:output}
\end{figure}

Na obrázku \ref{fig:output} je vidět konkrétní vstup a výstup naší aplikace. Zobrazí se název článku dotazu, jméno jeho autora a také samotný text článku. Naše aplikace dále uživateli nabídne seznam 10 nejpodobnějších článků i s určenou kosinovou podobností v procentech. Je vidět, že aplikace vrací víceméně přesně to, co bychom čekali. Jako nejpodobnější vrátila téměř shodný článek taktéž informující, že firma SpaceX vypustila do vesmíru raketu. Další navrhované články se také týkají firmy SpaceX, vesmíru nebo businessu. 

\section{Experimentální sekce}

\subsection{Určení optimálního počtu konceptů}

\begin{figure}[h] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.7\columnwidth]{images/singular_values.png}
	\caption{Důležitost konceptů}
	\label{fig:concepts}
\end{figure}

Vezmeme si singulární hodnoty, které nám vrátil singulární rozklad v rovnici \ref{eq:svd}. Vizualizujeme-li si v grafu \ref{fig:concepts}, jak klesá poměr nejvyšší singulární hodnoty vůči zbylým singulárním hodnotám, vidíme z toho také, jak klesá důležitost konceptů v datasetu. Počet konceptů $k$ v naší aplikaci tedy podle tohoto grafu určíme jako 200.

\subsection{Porovnání vlivu LSI na kvalitu výsledků vyhledávání s ohledem na výskyt synonym a homonym}

Pro zjištění kvality výsledků vyhledávání s ohledem na výskyt synonym a homonym jsme do datasetu přidali 4 články.

\subsubsection{Synonyma}

Jak si náš model poradí se synonymy jsme testovali na článcích \emph{"Silicon Valley’s favorite magician reimagines his act in the age of Zoom"} a \emph{"Leading NYC Illusionist Launches Virtual Magic Shows"}. Testovali jsme tedy anglická synonyma \emph{illusionist} a \emph{magician}.

\begin{figure}[h] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.8\columnwidth]{images/synonyms_0.png}
	\includegraphics[width=0.8\columnwidth]{images/synonyms_1.png}
	\caption{Synonyma}
	\label{fig:synonyms}
\end{figure}

\bigskip

Na obrázku \ref{fig:synonyms} je vidět, že náš model synonyma v článcích identifikoval správně. V prvním článku se sice ani jednou přímo nevyskytuje slovo \emph{magician} a v druhém ani jednou slovo \emph{illusionist}, ale v obou se vyskytují slova se slovním záladem \emph{magic} nebo například slova \emph{show} a \emph{audience} a model tak články správně identifikoval jako podobné.

\subsubsection{Homonyma}

K testování jak si náš článek poradí s homonymy jsme použili články \emph{"Little Richard, rock 'n' roll pioneer, has died at 87"} a \emph{"Scientists found bacteria inside rocks — here’s what that could mean for life on Mars"}, které oba obsahují slovo \emph{rock} ovšem jednou ve významu skály a jednou ve významu rockového muzikanta.

\begin{figure}[h] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.8\columnwidth]{images/homonyms_0.png}
	\includegraphics[width=0.8\columnwidth]{images/homonyms_1.png}
	\caption{Homonyma}
	\label{fig:homonyms}
\end{figure}

\bigskip

Na obrázku \ref{fig:homonyms} je vidět, že u prvního článku, týkajícího se vlivu objevu bakterie v podmořských skalách na možnost života na Marsu, si model poradil opravdu dobře a vrátil nám články týkající se vesmíru nebo přírody.
\bigskip

U druhého článku týkajícího se smrti rockového hudebníka nám model také velmi správně správně jako nejpodobnější vrátil články týkající se dalších mrtvých hudebníků. Dále vrátil také relevantní články o hudbě a celebritách.

\subsection{Porovnání průchodu pomocí LSI vektorového modelu se sekvenčním průchodem databáze s ohledem na čas vykonání dotazu}

Pro odsimulování sekvenčního průchodu databází nastavíme počet konceptů stejný jako počet dokumentů databáze. Toho docílíme tak, že v souboru \emph{server/lsa\_config.json} nastavíme parametr $k$ na 0.

\begin{figure}[h] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=1\columnwidth]{images/measure_time.png}
	\caption{Porovnání času vykonání dotazu při sekvenčním průchodu databáze}
	\label{fig:measure_time}
\end{figure}

\bigskip

Na obrázku \ref{fig:measure_time} je vidět, že při sekvenčním průchodu databází se čas vykonání dotazu zpomalí asi o
0.05 sekund. Při testování v prohlížeči je však rozdíl v rychlosti načítání stránky zobrazující obsah článku a 10 jemu nejpodobnějších článku mnohem výraznější, kolem 4 sekund.

\subsection{Vliv různých vnitřních parametrů na výkon algoritmu (změna počtu konceptů, změna počtu extrahovaných termů, použití lemmatizace namísto stemmingu, odstranění číslovek při preprocesingu, použití jiného vzorce na výpočet vah termů)}

\begin{figure}[h] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=1\columnwidth]{images/homonym_bad.png}
	\caption{Špatné určení článku obsahujícího homonymum jako podobného}
	\label{fig:homonym_bad}
\end{figure}

\subsubsection{Změna počtu konceptů}
Při snížení počtu konceptů na 100 vrací model stále relevantní výsledky a také určování homonym a synonym funguje správně. Na obrázku \ref{fig:homonym_bad} je vidět, že při zvýšení počtu konceptů na 300 už model špatně určí jeden článek obsahující homonymum jako podobný. Tato chyba je se zvyšujícím se počtem konceptů čím dál častější.

\subsubsection{Změna počtu extrahovaných termů}

V naší aplikaci ve funkci na redukci termů (o které píšeme v části \ref{subsec:term_weights}) odstraňujeme pomocí parametru \emph{min\_df} pouze termy vyskytující se pouze v jednom dokumentu. Parametr \emph{max\_df} necháváme defultně nastavený na 1, po preprocesingu nemáme v našem datasetu termy vyskytující se v hodně dokumentech. Parametr \emph{max\_terms} také necháváme defaultně nastavený na 0. Uřednostňujeme nechat si v datasetu všechny důležité termy a mít přesnější výsledky před rychlejší dobou běhu.

\subsubsection{Použití lemmatizace namísto stemmingu}

\begin{figure}[h] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=1\columnwidth]{images/preprocesing_time.png}
	\caption{Čas preprocesingu při použití stemmingu versus při použití lemmatizace}
	\label{fig:preprocesing_time}
\end{figure}

Na obrázku \ref{fig:preprocesing_time} je vidět, že při použití lemmatizace namísto stemmingu se čas preprocesingu více než zčtyřnásobí na téměř 30 sekund. Uděláme-li si graf důležitosti konceptů jako na obrázku \ref{fig:concepts} vyjde při použití stemmingu i lemmatizace úplně stejný. V naší aplikaci jsme se rozhodli používat lemmatizaci a upřednostnit tak přesnější převádění slov z dokumentů na jednotlivé termy na úkor delšího času preprocesingu.

\subsubsection{Odstranění číslovek při preprocesingu}

Číslovky z dokumentů jsme se v naší aplikaci rozhodli odstraňovat, jelikož nám pak aplikace vrací o něco lepší výstupy. Číslovky v našem datasetu novinových článků nejsou většinou pro podobnost moc relevantní spíše naopak.

\subsubsection{Použití jiného vzorce pro výpočet vah termů}

TODO

\section{Diskuze}


\section{Závěr}

\end{document}
